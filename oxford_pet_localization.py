# -*- coding: utf-8 -*-
"""Oxford_Pet_Localization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HV5CFFetaKmH99VycgYJMsfK9-dek2yh
"""

## library import
import tensorflow as tf
from tensorflow import keras
import numpy as np
import os
import re
from PIL import Image
import shutil
import xml.etree.ElementTree as et
import random
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

print(tf.__version__)
print(keras.__version__)

## google drive에서 압축된 dataset download
import gdown
url = 'https://drive.google.com/uc?id=1dIR9ANjUsV9dWa0pS9J0c2KUGMfpIRG0'
fname = 'oxford_pet.zip'
gdown.download(url, fname, quiet=False)

## oxford_pet.zip이 보이는지 확인
!ls -l

## 압축풀기
!unzip -q oxford_pet.zip -d oxford_pet

## 압축이 풀린 directory 확인
!ls oxford_pet

## directory 정보
cur_dir = os.getcwd()
data_dir = os.path.join(cur_dir, 'oxford_pet')
image_dir = os.path.join(data_dir, 'images')
bbox_dir = os.path.join(data_dir, 'annotations', 'xmls')
seg_dir = os.path.join(data_dir, 'annotations', 'trimaps')

## image file 수 확인
image_files = [fname for fname in os.listdir(image_dir) if os.path.splitext(fname)[-1] == '.jpg']
print(len(image_files))

## localization을 위한 annotation이 되어 있는 file의 수 확인
bbox_files = [fname for fname in os.listdir(bbox_dir) if os.path.splitext(fname)[-1] == '.xml']
len(bbox_files)

## image file들을 읽어서 channel이 3이 아닌 image는 삭제, xml도 같이 삭제
for image_file in image_files:
  image_path = os.path.join(image_dir, image_file)
  bbox_file = os.path.splitext(image_file)[0]+'.xml'
  bbox_path = os.path.join(bbox_dir, bbox_file)
  image = Image.open(image_path)
  image_mode = image.mode
  if image_mode != 'RGB':
    print(image_file, image_mode)
    image = np.asarray(image)
    print(image.shape)
    os.remove(image_path)
    try:
      os.remove(bbox_path)
    except FileNotFoundError:
      pass

## image file 수 확인
image_files = [fname for fname in os.listdir(image_dir) if os.path.splitext(fname)[-1] == '.jpg']
print(len(image_files))

## localization을 위한 annotation이 되어 있는 file의 수 확인
bbox_files = [fname for fname in os.listdir(bbox_dir) if os.path.splitext(fname)[-1] == '.xml']
len(bbox_files)

class_list = set()
for image_file in image_files:
    file_name = os.path.splitext(image_file)[0]
    class_name = re.sub('_\d+', '', file_name)
    class_list.add(class_name)
class_list = list(class_list)
print(len(class_list))

class_list.sort()
class_list

class2idx = {cls:idx for idx, cls in enumerate(class_list)}
class2idx

bbox_files[:20]

IMG_SIZE = 224
N_BBOX = len(bbox_files)
N_TRAIN = 3000
N_VAL = N_BBOX - N_TRAIN

## TFRecord 저장할 directory와 file 경로 설정
tfr_dir = os.path.join(data_dir, 'tfrecord')
os.makedirs(tfr_dir, exist_ok=True)

tfr_train_dir = os.path.join(tfr_dir, 'loc_train.tfr')
tfr_val_dir = os.path.join(tfr_dir, 'loc_val.tfr')

## TFRecord writer 생성
writer_train = tf.io.TFRecordWriter(tfr_train_dir)
writer_val = tf.io.TFRecordWriter(tfr_val_dir)

# The following functions can be used to convert a value to a type compatible
# with tf.Example.

def _bytes_feature(value):
  """Returns a bytes_list from a string / byte."""
  if isinstance(value, type(tf.constant(0))):
    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _float_feature(value):
  """Returns a float_list from a float / double."""
  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))

def _int64_feature(value):
  """Returns an int64_list from a bool / enum / int / uint."""
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

shuffle_list = list(range(N_BBOX))
random.shuffle(shuffle_list)

train_idx_list = shuffle_list[:N_TRAIN]
val_idx_list = shuffle_list[N_TRAIN:]

shuffle_list

for idx in train_idx_list:
    
  bbox_file = bbox_files[idx]
  bbox_path = os.path.join(bbox_dir, bbox_file)

  tree = et.parse(bbox_path)
  width = float(tree.find('./size/width').text)
  height = float(tree.find('.size/height').text)
  xmin = float(tree.find('./object/bndbox/xmin').text)
  ymin = float(tree.find('./object/bndbox/ymin').text)
  xmax = float(tree.find('./object/bndbox/xmax').text)
  ymax = float(tree.find('./object/bndbox/ymax').text)
  xc = (xmin + xmax) / 2.
  yc = (ymin + ymax) / 2.
  x = xc / width
  y = yc / height
  w = (xmax - xmin) / width
  h = (ymax - ymin) / height

  file_name = os.path.splitext(bbox_file)[0]
  image_file = file_name + '.jpg'
  image_path = os.path.join(image_dir, image_file)
  image = Image.open(image_path)
  image = image.resize((IMG_SIZE, IMG_SIZE))
  bimage = image.tobytes()

  class_name = re.sub('_\d+', '', file_name)
  class_num = class2idx[class_name]

  if file_name[0].islower(): # dog
    bi_cls_num = 0
  else: # cat
    bi_cls_num = 1
  
  example = tf.train.Example(features=tf.train.Features(feature={
      'image': _bytes_feature(bimage),
      'cls_num': _int64_feature(class_num),
      'bi_cls_num': _int64_feature(bi_cls_num),
      'x': _float_feature(x),
      'y': _float_feature(y),
      'w': _float_feature(w),
      'h': _float_feature(h)
  }))
  writer_train.write(example.SerializeToString())

writer_train.close()

for idx in val_idx_list:
  bbox_file = bbox_files[idx]
  bbox_path = os.path.join(bbox_dir, bbox_file)

  tree = et.parse(bbox_path)
  width = float(tree.find('./size/width').text)
  height = float(tree.find('.size/height').text)
  xmin = float(tree.find('./object/bndbox/xmin').text)
  ymin = float(tree.find('./object/bndbox/ymin').text)
  xmax = float(tree.find('./object/bndbox/xmax').text)
  ymax = float(tree.find('./object/bndbox/ymax').text)
  xc = (xmin + xmax) / 2.
  yc = (ymin + ymax) / 2.
  x = xc / width
  y = yc / height
  w = (xmax - xmin) / width
  h = (ymax - ymin) / height

  file_name = os.path.splitext(bbox_file)[0]
  image_file = file_name + '.jpg'
  image_path = os.path.join(image_dir, image_file)
  image = Image.open(image_path)
  image = image.resize((IMG_SIZE, IMG_SIZE))
  bimage = image.tobytes()

  class_name = re.sub('_\d+', '', file_name)
  class_num = class2idx[class_name]

  if file_name[0].islower(): # dog
    bi_cls_num = 0
  else: # cat
    bi_cls_num = 1
  
  example = tf.train.Example(features=tf.train.Features(feature={
      'image': _bytes_feature(bimage),
      'cls_num': _int64_feature(class_num),
      'bi_cls_num': _int64_feature(bi_cls_num),
      'x': _float_feature(x),
      'y': _float_feature(y),
      'w': _float_feature(w),
      'h': _float_feature(h)
  }))
  writer_val.write(example.SerializeToString())

writer_val.close()

## Hyper Parameters
N_CLASS = len(class_list)
N_EPOCHS = 70
N_BATCH = 40
IMG_SIZE = 224
learning_rate = 0.0001
steps_per_epoch = N_TRAIN / N_BATCH
validation_steps = int(np.ceil(N_VAL / N_BATCH))

## tfrecord file을 data로 parsing해주는 function
def _parse_function(tfrecord_serialized):
    features={'image': tf.io.FixedLenFeature([], tf.string),
              'cls_num': tf.io.FixedLenFeature([], tf.int64),
              'bi_cls_num': tf.io.FixedLenFeature([], tf.int64),
              'x': tf.io.FixedLenFeature([], tf.float32),
              'y': tf.io.FixedLenFeature([], tf.float32),
              'w': tf.io.FixedLenFeature([], tf.float32),
              'h': tf.io.FixedLenFeature([], tf.float32)              
             }
    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)
    
    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)    
    image = tf.reshape(image, [IMG_SIZE, IMG_SIZE, 3])
    image = tf.cast(image, tf.float32)/255.

    cls_label = tf.cast(parsed_features['cls_num'], tf.int64)
    bi_cls_label = tf.cast(parsed_features['bi_cls_num'], tf.int64)
    
    x = tf.cast(parsed_features['x'], tf.float32)
    y = tf.cast(parsed_features['y'], tf.float32)
    w = tf.cast(parsed_features['w'], tf.float32)
    h = tf.cast(parsed_features['h'], tf.float32)
    gt = tf.stack([x, y, w, h], -1)
    
    return image, gt

## train dataset 만들기
train_dataset = tf.data.TFRecordDataset(tfr_train_dir)
train_dataset = train_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)
train_dataset = train_dataset.shuffle(buffer_size=N_TRAIN).prefetch(
    tf.data.experimental.AUTOTUNE).batch(N_BATCH).repeat()

## validation dataset 만들기
val_dataset = tf.data.TFRecordDataset(tfr_val_dir)
val_dataset = val_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)
val_dataset = val_dataset.batch(N_BATCH).repeat()

## train dataset에서 1개의 image와 bbox를 읽어서 확인
for image, gt in train_dataset.take(3):
    
    '''그림을 그리기 위해서 bbox의 왼쪽 위 꼭지점 좌표를 계산하고, 
    xmin, ymin, w, h 각각을 image size에 맞게 scaling'''
    x = gt[:,0]
    y = gt[:,1]
    w = gt[:,2]
    h = gt[:,3]
    xmin = x[0].numpy() - w[0].numpy()/2.
    ymin = y[0].numpy() - h[0].numpy()/2.
    rect_x = int(xmin * IMG_SIZE)
    rect_y = int(ymin * IMG_SIZE)
    rect_w = int(w[0].numpy() * IMG_SIZE)
    rect_h = int(h[0].numpy() * IMG_SIZE)
    
    ## 그림 그리기
    rect = Rectangle((rect_x, rect_y), rect_w, rect_h, fill=False, color='red')
    plt.axes().add_patch(rect)
    plt.imshow(image[0])
    plt.show()

# Sequential API를 사용하여 model 구성
def create_model():
    model = keras.Sequential()
    model.add(keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='SAME', 
                                  input_shape=(IMG_SIZE, IMG_SIZE, 3)))
    model.add(keras.layers.MaxPool2D(padding='SAME'))
    model.add(keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='SAME'))
    model.add(keras.layers.MaxPool2D(padding='SAME'))
    model.add(keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='SAME'))
    model.add(keras.layers.MaxPool2D(padding='SAME'))
    model.add(keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='SAME'))
    model.add(keras.layers.MaxPool2D(padding='SAME'))
    model.add(keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='SAME'))
    model.add(keras.layers.MaxPool2D(padding='SAME'))
    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(1024, activation='relu'))
    model.add(keras.layers.Dropout(0.4))
    model.add(keras.layers.Dense(4, activation='sigmoid'))
    return model

def loss_fn(y_true, y_pred):
  return keras.losses.MeanSquaredError()(y_true, y_pred)

## Create model, compile & summary
model = create_model()
model.summary()

## learning rate scheduing
lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,
                                                          decay_steps=steps_per_epoch*10,
                                                          decay_rate=0.5,
                                                          staircase=True)
## optimizer는 RMSprop, loss는 mean squared error 사용
model.compile(keras.optimizers.RMSprop(lr_schedule), loss=loss_fn)

## Train
model.fit(train_dataset, steps_per_epoch=steps_per_epoch,
         epochs=N_EPOCHS,
         validation_data=val_dataset,
         validation_steps=validation_steps)

## 예측한 bounding box와 ground truth box를 image에 같이 표시
## 정답은 빨간색 box, 예측은 파란색 box
idx = 0
num_imgs = validation_steps
for val_data, val_gt in val_dataset.take(num_imgs):
    ## 정답 box 그리기
    x = val_gt[:,0]
    y = val_gt[:,1]
    w = val_gt[:,2]
    h = val_gt[:,3]
    xmin = x[idx].numpy() - w[idx].numpy()/2.
    ymin = y[idx].numpy() - h[idx].numpy()/2.
    rect_x = int(xmin * IMG_SIZE)
    rect_y = int(ymin * IMG_SIZE)
    rect_w = int(w[idx].numpy() * IMG_SIZE)
    rect_h = int(h[idx].numpy() * IMG_SIZE)
    
    rect = Rectangle((rect_x, rect_y), rect_w, rect_h, fill=False, color='red')
    plt.axes().add_patch(rect)
    
    ## 예측 box 그리기
    ## validation set에 대해서 bounding box 예측
    prediction = model.predict(val_data)
    pred_x = prediction[:,0]
    pred_y = prediction[:,1]
    pred_w = prediction[:,2]
    pred_h = prediction[:,3]
    pred_xmin = pred_x[idx] - pred_w[idx]/2.
    pred_ymin = pred_y[idx] - pred_h[idx]/2.
    pred_rect_x = int(pred_xmin * IMG_SIZE)
    pred_rect_y = int(pred_ymin * IMG_SIZE)
    pred_rect_w = int(pred_w[idx] * IMG_SIZE)
    pred_rect_h = int(pred_h[idx] * IMG_SIZE)
    
    pred_rect = Rectangle((pred_rect_x, pred_rect_y), pred_rect_w, pred_rect_h,
                         fill=False, color='blue')
    plt.axes().add_patch(pred_rect)
    
    ## image와 bbox 함께 출력
    plt.imshow(val_data[idx])
    plt.show()

avg_iou = 0
num_imgs = validation_steps
res = N_VAL % N_BATCH #5
for i, (val_data, val_gt) in enumerate(val_dataset.take(num_imgs)):
    ## 정답 box 그리기
    flag = (i == validation_steps-1)
    x = val_gt[:,0]
    y = val_gt[:,1]
    w = val_gt[:,2]
    h = val_gt[:,3]
    prediction = model.predict(val_data)
    pred_x = prediction[:,0]
    pred_y = prediction[:,1]
    pred_w = prediction[:,2]
    pred_h = prediction[:,3]
    for idx in range(N_BATCH):
      if (flag):
        if idx == res:
          flag = False
          break          
      xmin = int((x[idx].numpy() - w[idx].numpy()/2.)*IMG_SIZE)
      ymin = int((y[idx].numpy() - h[idx].numpy()/2.)*IMG_SIZE)
      xmax = int((x[idx].numpy() + w[idx].numpy()/2.)*IMG_SIZE)
      ymax = int((y[idx].numpy() + h[idx].numpy()/2.)*IMG_SIZE)

      pred_xmin = int((pred_x[idx] - pred_w[idx]/2.)*IMG_SIZE)
      pred_ymin = int((pred_y[idx] - pred_h[idx]/2.)*IMG_SIZE)
      pred_xmax = int((pred_x[idx] + pred_w[idx]/2.)*IMG_SIZE)
      pred_ymax = int((pred_y[idx] + pred_h[idx]/2.)*IMG_SIZE)

      if xmin > pred_xmax or xmax < pred_xmin:        
        continue
      if ymin > pred_ymax or ymax < pred_ymin:        
        continue
      w_union = np.max((xmax, pred_xmax)) - np.min((xmin, pred_xmin))
      h_union = np.max((ymax, pred_ymax)) - np.min((ymin, pred_ymin))
      w_inter = np.min((xmax, pred_xmax)) - np.max((xmin, pred_xmin))
      h_inter = np.min((ymax, pred_ymax)) - np.max((ymin, pred_ymin))

      w_sub1 = np.abs(xmax - pred_xmax)
      h_sub1 = np.abs(ymax - pred_ymax)
      w_sub2 = np.abs(xmin - pred_xmin)
      h_sub2 = np.abs(ymin - pred_ymin)

      iou = (w_inter * h_inter) / ((w_union * h_union) - (w_sub1 * h_sub1) - (w_sub2 * h_sub2))
      avg_iou += iou / N_VAL

print(avg_iou)

from tensorflow.keras import optimizers
from tensorflow.keras.applications.resnet import ResNet152
from tensorflow.keras.layers import Conv2D, ReLU, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D,Concatenate,GlobalMaxPool2D

resnet152=ResNet152(include_top=False, weights='imagenet', input_shape=(IMG_SIZE,IMG_SIZE,3))

model_res=keras.models.Sequential()
model_res.add(resnet152)
model_res.add(GlobalAveragePooling2D())
model_res.add(Dense(256))
model_res.add(BatchNormalization())
model_res.add(ReLU())
model_res.add(Dense(64))
model_res.add(BatchNormalization())
model_res.add(ReLU())
model_res.add(Dense(4,activation='sigmoid'))
model_res.summary()

model_res.compile(keras.optimizers.Adam(lr_schedule),loss=loss_fn)

history_res = model_res.fit(
    train_dataset,
    epochs=20,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_dataset,
    validation_steps=validation_steps
)

idx=0
num_imgs=validation_steps
for val_data, val_gt in val_dataset.take(num_imgs):
  # 정답 box 그리기
  x=val_gt[:,0]
  y=val_gt[:,1]
  w=val_gt[:,2]
  h=val_gt[:,3]
  xmin=x[idx].numpy()-w[idx].numpy()/2.
  ymin=y[idx].numpy()-h[idx].numpy()/2.
  rect_x=int(xmin*IMG_SIZE)
  rect_y=int(ymin*IMG_SIZE)
  rect_w=int(w[idx].numpy()*IMG_SIZE)
  rect_h=int(h[idx].numpy()*IMG_SIZE)

  rect=Rectangle((rect_x,rect_y),rect_w,rect_h,fill=False,color='red')
  plt.axes().add_patch(rect)
  
  # 예측 box 그리기
  # validation set에 대해서 bounding box 예측
  prediction=model_res.predict(val_data)
  pred_x=prediction[:,0]
  pred_y=prediction[:,1]
  pred_w=prediction[:,2]
  pred_h=prediction[:,3]
  pred_xmin=pred_x[idx]-pred_w[idx]/2.
  pred_ymin=pred_y[idx]-pred_h[idx]/2.
  pred_rect_x=int(pred_xmin*IMG_SIZE)
  pred_rect_y=int(pred_ymin*IMG_SIZE)
  pred_rect_w=int(pred_w[idx]*IMG_SIZE)
  pred_rect_h=int(pred_h[idx]*IMG_SIZE)

  pred_rect=Rectangle((pred_rect_x,pred_rect_y),pred_rect_w,pred_rect_h,fill=False,color='blue')
  plt.axes().add_patch(pred_rect)

  plt.imshow(val_data[idx])
  plt.show()

avg_iou = 0
num_imgs = validation_steps
res = N_VAL % N_BATCH
for i, (val_data, val_gt) in enumerate(val_dataset.take(num_imgs)):
  flag=(i==validation_steps -1)
  x=val_gt[:,0]
  y=val_gt[:,1]
  w=val_gt[:,2]
  h=val_gt[:,3]
  prediction=model_res.predict(val_data)
  pred_x=prediction[:,0]
  pred_y=prediction[:,1]
  pred_w=prediction[:,2]
  pred_h=prediction[:,3]
  for idx in range(N_BATCH):
    if (flag):
      if idx==res:
        flag=False
        break
    xmin=int((x[idx].numpy()-w[idx].numpy()/2.)*IMG_SIZE)
    ymin=int((y[idx].numpy()-h[idx].numpy()/2.)*IMG_SIZE)
    xmax=int((x[idx].numpy()+w[idx].numpy()/2.)*IMG_SIZE)
    ymax=int((y[idx].numpy()+h[idx].numpy()/2.)*IMG_SIZE)

    pred_xmin=int((pred_x[idx]-pred_w[idx]/2.)*IMG_SIZE)
    pred_ymin=int((pred_y[idx]-pred_h[idx]/2.)*IMG_SIZE)
    pred_xmax=int((pred_x[idx]+pred_w[idx]/2.)*IMG_SIZE)
    pred_ymax=int((pred_y[idx]+pred_h[idx]/2.)*IMG_SIZE)

    if xmin>pred_xmax or xmax<pred_xmin:
      continue
    if ymin>pred_ymax or ymax<pred_ymin:
      continue
    w_union=np.max((xmax,pred_xmax))-np.min((xmin,pred_xmin))
    h_union=np.max((ymax,pred_ymax))-np.min((ymin,pred_ymin))
    w_inter=np.min((xmax,pred_xmax))-np.max((xmin,pred_xmin))
    h_inter=np.min((ymax,pred_ymax))-np.max((ymin,pred_ymin))

    w_sub1=np.abs(xmax-pred_xmax)
    h_sub1=np.abs(ymax-pred_ymax)
    w_sub2=np.abs(xmin-pred_xmin)
    h_sub2=np.abs(ymin-pred_ymin)

    iou=(w_inter*h_inter) / ((w_union*h_union)-(w_sub1*h_sub1)-(w_sub2*h_sub2))
    avg_iou +=iou/N_VAL

print(avg_iou)